{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c757715",
   "metadata": {},
   "source": [
    "# HulC for Online Algorithms\n",
    "## Introduction\n",
    "For big data problems, M-estimators are often obtained using online algorithms such as the stochastic gradient descent (SGD) or many of its variants. These algorithms have a low cost per iteration and only takes $n$ iterations where $n$ is the number of observations. The vanilla SGD has been extended and generalized in several ways for improvement. For example, the implicit SGD of [Toulis and Airoldi](https://drive.google.com/file/d/1aC6ZHZF-_YDw4T8cwYlmfwSqL9-LREyf/view) (2017, Annals of Statistics) provides implicit updates on the parameter values and brings more stability compared to the vanilla SGD. The ROOT-SGD method of [Li, Mou, Wainwright, and Jordon](https://arxiv.org/pdf/2008.12690.pdf) (2020, ArXiv:2008.12690) improves the vanilla SGD in terms of its finite sample behavior as well as reduce the order of higher order terms for asymptotic normality. The gradient-free SGD method of [Chen, Lai, Li, and Zhang](https://arxiv.org/pdf/2102.03389.pdf) (2021, ArXiv:2102.03389) relaxes the requirement explicitly known unbiased estimator of gradient for SGD. They only use function values to compute the gradient by Gaussian perturbation. These online algorithms nicely tackle the problem of estimation from a computational perspective. However, inference via confidence intervals using these online algorithms is relatively not as well understood. For all the algorithms mentioned above including the vanilla SGD, it is known that under certain regularity conditions, the average of the iterates has an asymptotic Gaussian distribution centered at the true parameter value. For this result for vanilla SGD, see [Polyak and Juditsky](http://www.meyn.ece.ufl.edu/archive/spm_files/Courses/ECE555-2011/555media/poljud92.pdf) (1992, SIAM J. Control and Optimization). The inference problem based on these online algorithms has received a lot of attention in the recent times. There are two main lines of research:\n",
    "- Estimating the asymptotic covariance matrix and construct the usual Wald intervals;\n",
    "- Run a parallel perturbed online algorithms so as to replicate the distribution of the original estimator. This is similar to multiplier bootstrap.\n",
    "\n",
    "## SGD and asymptotic normality\n",
    "Let us now discuss the vanilla SGD and discuss its asymptotic normality. Let $\\theta^{\\star}\\in\\mathbb{R}^d$ be defined as\n",
    "\\begin{equation*}\n",
    "\\theta^{\\star} := \\underset{\\theta\\in\\mathbb{R}^d}{\\text{argmin}} F(\\theta),\\quad\\mbox{where}\\quad F(\\theta) = \\mathbb{E}[f(Z; \\theta)],\n",
    "\\end{equation*}\n",
    "for a random variable $Z$ in some measurable space. Let $Z_1, Z_2, \\ldots$ be a sequence of independent and identically distributed random variables observed sequentially. If $\\theta_0$ denotes a given starting point, the SGD iterate at the $t$-th iterate is given by\n",
    "\\begin{equation*}\n",
    "\\theta_t = \\theta_{t-1} - \\eta_t\\nabla f(Z_t; \\theta_{t-1}),\n",
    "\\end{equation*}\n",
    "where $\\nabla f(z; \\theta) = \\partial f(z; \\theta)/\\partial\\theta$ denotes the gradient of $\\theta\\mapsto f(z; \\theta)$.\n",
    "In general, the step size $\\eta_t$ is chosen to decrease with $t$. At time $T$, the algorithm can return either the last iterate $\\theta_T$ or the average iterate $\\bar{\\theta}_T = T^{-1}\\sum_{t=1}^T \\theta_t$. The averaging is called Polyak--Ruppert averaging. The results of [Polyak and Juditsky](http://www.meyn.ece.ufl.edu/archive/spm_files/Courses/ECE555-2011/555media/poljud92.pdf) imply that the average iterate is a \"better\" estimator of $\\theta^{\\star}$ than the last iterate. In the following, we only discuss the properties of the average iterate $\\bar{\\theta}_T$. \n",
    "### Asymptotic Normality\n",
    "Recall $F(\\theta) = \\mathbb{E}[f(Z; \\theta)]$. Define $\\xi_t = \\nabla f(\\theta_{t-1}, X_t) - \\nabla F(\\theta_{t-1})$. Because $X_t$ is independent of $\\theta_{t-1}$, $\\mathbb{E}[\\xi_t|X_1, \\ldots, X_{t-1}] = 0$. By a one step Taylor expansion, \n",
    "\\begin{align*}\n",
    "\\nabla F(\\theta_{t-1}) ~&=~ \\nabla F(\\theta^\\star) + \\nabla_2 F(\\theta^\\star)(\\theta_{t-1} - \\theta^\\star) + R_{t-1}\\\\\n",
    "~&=~ \\nabla_2 F(\\theta^\\star)(\\theta_{t-1} - \\theta^\\star) + R_{t-1},\n",
    "\\end{align*}\n",
    "with a second order remainder $R_{t-1}$.\n",
    "This implies that $\\xi_t = \\nabla f(\\theta_{t-1}, X_t) - J(\\theta_{t-1} - \\theta^\\star) - R_{t-1}$, where $J = \\nabla_2 F(\\theta^\\star)$. Therefore,\n",
    "\\begin{equation*}\n",
    "\\theta_t = \\theta_{t-1} - \\eta_t[\\xi_t - J(\\theta_{t-1} - \\theta^\\star) - R_{t-1}],\n",
    "\\end{equation*}\n",
    "which yields\n",
    "\\begin{equation*}\n",
    "\\frac{1}{T}\\sum_{t = 1}^T \\frac{\\theta_t - \\theta_{t-1}}{\\eta_t} = -\\frac{1}{T}\\sum_{t=1}^T \\xi_t + J\\left(\\frac{1}{T}\\sum_{t = 1}^T \\theta_{t-1} - \\theta^\\star\\right) + \\frac{1}{T}\\sum_{t=1}^T R_{t-1}.\n",
    "\\end{equation*}\n",
    "Rearranging, we conclude\n",
    "\\begin{align*}\n",
    "        \\left( \\frac{1}{T}\\sum_{t=1}^T \\theta_t - \\theta^\\star \\right)\n",
    "        & = \\frac{1}{T}\\sum_{t=1}^T(\\theta_t -  \\theta_{t-1} + \\theta_{t-1} -\\theta^\\star)\\\\\n",
    "        & = \\frac{1}{T}\\sum_{t=1}^T(\\theta_{t-1} -\\theta^\\star) + \\frac{\\theta_T - \\theta_0}{T}\\\\\n",
    "        & = \\frac{1}{T}\\sum_{t=1}^T J^{-1}\\xi_t - \\frac{1}{T}\\sum_{t=1}^T \\frac{J^{-1}(\\theta_t - \\theta_{t-1})}{\\eta_t} - \\frac{1}{T}\\sum_{t=1}^T J^{-1}R_{t-1} + \\frac{\\theta_T - \\theta_0}{T}.\n",
    "\\end{align*}\n",
    "Hence,\n",
    "\\begin{equation*}\\label{eq:SGD-influence}\n",
    "\\sqrt{T}\\left\\|\\frac{1}{T}\\sum_{t=1}^T \\theta_t - \\theta^\\star - \\frac{1}{T}\\sum_{t=1}^T J^{-1}\\xi_t\\right\\| \\le \\sqrt{T}\\|\\mathrm{Rem}_T\\|,\n",
    "\\end{equation*}\n",
    "where\n",
    "\\begin{equation*}\n",
    "\\mathrm{Rem}_T ~:=~ \\frac{1}{T}\\sum_{t=1}^T \\frac{J^{-1}(\\theta_t - \\theta_{t-1})}{\\eta_t} + \\frac{1}{T}\\sum_{t=1}^T J^{-1}R_{t-1} + \\frac{J^{-1}(\\theta_T - \\theta_0)}{T}\n",
    "\\end{equation*}\n",
    "The remainder term $T^{1/2}\\mathrm{Rem}_T$ can be proved to converge to zero if $\\eta_t \\gg t^{-1/2}$ and under quasi-convexity assumptions; see [Gower et al. (2019, PMLR)](http://proceedings.mlr.press/v97/qian19b) and [Moulines and Bach (2011, NeuRIPS)](https://papers.nips.cc/paper/2011/hash/40008b9a5380fcacce3976bf7c08af5b-Abstract.html) for details. Assuming $T^{1/2}\\mathrm{Rem}_T = o_p(1)$, we conclude from the martingale central limit theorem that\n",
    "\\begin{equation*}\n",
    "\\sqrt{T}(\\bar{\\theta}_T - \\theta^{\\star}) \\overset{d}{\\to} N(0, J^{-1}KJ^{-1}),\\quad\\mbox{where}\\quad K = \\mathbb{E}[\\nabla f(Z; \\theta^{\\star})(\\nabla f(Z; \\theta^{\\star}))^{\\top}].\n",
    "\\end{equation*}\n",
    "Also, see [Anastasiou, Balasubramanian, and Erdogdu (2019, PMLR)](http://proceedings.mlr.press/v99/anastasiou19a/anastasiou19a.pdf) for a Berry--Esseen bound quantifying the rate of convergence to normality. \n",
    "### Inference using SGD: Analogue of Wald's\n",
    "Given the asymptotic normality of the average iterate $\\bar{\\theta}_T$, performing inference is straightforward if one has access to an estimate of the asymptotic variance $J^{-1}KJ^{-1}$. In the batch setting where the data is available all at once and no restrictions on computation, one can estimate $J$ and $K$ via\n",
    "\\begin{equation*}\n",
    "\\widetilde{J} := \\frac{1}{T}\\sum_{t=1}^T \\nabla_2 f(Z_t; \\bar{\\theta}_T)\\quad\\mbox{and}\\quad \\widetilde{K} := \\frac{1}{T}\\sum_{t=1}^T \\nabla f(Z_t; \\bar{\\theta}_T)(\\nabla f(Z_i; \\bar{\\theta}))^{\\top}.\n",
    "\\end{equation*}\n",
    "In order to compute the estimators as above, one would need to run through the whole once more after obtaining $\\bar{\\theta}_T$, which could be computationally prohibitive. In order to avoid this additional computations, Section 4.1 of [Chen, Lee, Tong, and Zhang (2020, AoS)](https://projecteuclid.org/euclid.aos/1581930134) provides online plug-in estimators\n",
    "\\begin{equation*}\n",
    "\\widehat{J} := \\frac{1}{T}\\sum_{t = 1}^T \\nabla_2f(Z_t; \\theta_{t-1})\\quad\\mbox{and}\\quad \\widehat{K} := \\frac{1}{T}\\sum_{t=1}^T \\nabla f(Z_t; \\theta_{t-1})(\\nabla f(Z_t; \\theta_{t-1}))^{\\top}.\n",
    "\\end{equation*}\n",
    "The difference between the equations above is that $\\nabla_2 f(Z_t; \\bar{\\theta}_T)$ is replaced with $\\nabla_2 f(Z_t; \\theta_{t-1})$. While the former is unavailable at iteration $t$, the latter is readily available. Lemma 4.1 and Theorem 4.2 of [Chen, Lee, Tong, and Zhang (2020, AoS)](https://projecteuclid.org/euclid.aos/1581930134) show that $\\widehat{J}^{-1}\\widehat{K}\\widehat{J}^{-1}$ is a consistent estimator of $J^{-1}KJ^{-1}$. However, there are two main disadvantages:\n",
    "- We need the function $\\theta\\mapsto f(Z; \\theta)$ is twice differentiable in order to compute $\\widehat{J}$. The consistency property of $\\widehat{J}$ in Lemma 4.1 of [Chen, Lee, Tong, and Zhang (2020, AoS)](https://projecteuclid.org/euclid.aos/1581930134) requires Lipschitz continuity of $\\theta\\mapsto \\nabla_2f(Z; \\theta)$; see Assumption 4.1 of the paper. Such an assumption is not required to prove asymptotic normality. Note that in the proof above for $\\bar{\\theta}_T$, we only need twice differentiability of $\\theta\\mapsto F(\\theta) = \\mathbb{E}[f(Z; \\theta)]$.\n",
    "- The rate of convergence of $\\widehat{J}^{-1}\\widehat{K}\\widehat{J}^{-1}$ is considerably slower compared to that of $\\widetilde{J}^{-1}\\widetilde{K}\\widetilde{J}^{-1}$. For the usual step size $\\eta_t \\propto t^{-1/2}$, the rate of convergence of $\\widehat{J}^{-1}\\widehat{K}\\widehat{J}^{-1}$ is $T^{-1/4}$, while the usual sandwich estimator attains an $T^{-1/2}$ rate.\n",
    "\n",
    "In Section 4.2 of [Chen, Lee, Tong, and Zhang (2020, AoS)](https://projecteuclid.org/euclid.aos/1581930134), the authors propose an alternate estimator, batch-means estimator, of $J^{-1}KJ^{-1}$ that avoids the Lipschitz continuity assumption on $\\theta\\mapsto\\nabla_2f(Z; \\theta)$. If $\\eta_t = \\eta t^{-\\alpha}$, set $e_k = ((k+1)T)^{1/(1-\\alpha)}, k = 0, 1, \\ldots, M$, then split the SGD iterates $\\theta_t, t \\ge 0$ into $M$ batches as\n",
    "\\begin{equation*}\n",
    "\\underbrace{\\{\\theta_{s_0}, \\ldots, \\theta_{e_0}\\}}_{0\\mbox{-th batch}},\\,\\underbrace{\\{\\theta_{s_1}, \\ldots, \\theta_{e_1}\\}}_{1\\mbox{-st batch}},\\,\\ldots,\\,\\underbrace{\\{\\theta_{s_M}, \\ldots, \\theta_{e_M}\\}}_{M\\mbox{-th batch}}.\n",
    "\\end{equation*}\n",
    "Here $s_0 = 1, s_{k} = e_{k-1} + 1, e_M = T$. Set $n_k = e_k - s_k + 1$. The batch-means estimator of $J^{-1}KJ^{-1}$ is given by \n",
    "\\begin{equation*}\n",
    "\\frac{1}{M}\\sum_{k= 1}^M n_k(\\bar{\\theta}_{n_k} - \\bar{\\theta}_{M})(\\bar{\\theta}_{n_k} - \\bar{\\theta}_M)^{\\top},\n",
    "\\end{equation*}\n",
    "where $\\bar{\\theta}_{n_k} = n_k^{-1}\\sum_{t=s_k}^{e_k}\\theta_t$ is the mean of the iterates for the $k$-th batch and $\\bar{\\theta}_M = (e_M - e_0)^{-1}\\sum_{i=s_1}^{e_M} \\theta_t$ is the mean of all the iterates except for the $0$-th batch. Theorem 4.3 and Corollary 4.5 of [Chen, Lee, Tong, and Zhang (2020, AoS)](https://projecteuclid.org/euclid.aos/1581930134) show that the batch means estimator is consistent for $J^{-1}KJ^{-1}$, but only attains an $T^{-1/4}$ rate of convergence. It must be mentioned that the batch means estimator avoids the computation of inverse of $\\widehat{J}$. The batch means estimator involves additional tuning parameters to choose such as the number of entries in each batch and the number of batches. From the simulations of [Chen, Lee, Tong, and Zhang (2020, AoS)](https://projecteuclid.org/euclid.aos/1581930134), it seems that the batch means estimator substantially underestimates the variance of the SGD estimator yielding confidence intervals that undercover the true parameter. Recently, [Zhu, Chen, and Wu (2021, JASA)](https://www.tandfonline.com/doi/abs/10.1080/01621459.2021.1933498?src=&journalCode=uasa20) provides an alternative batch-means estimator of the asymptotic variance that does not require storing the iterates. \n",
    "\n",
    "With the consistent estimator of variance in hand, one can use the Wald intervals centered at the averate iterate estimator.\n",
    "### Inference using SGD: Analogue of Bootstrap \n",
    "Citing the deficiencies of the variance estimation of [Chen, Lee, Tong, and Zhang (2020, AoS)](https://projecteuclid.org/euclid.aos/1581930134), [Fang, Xu, and Yang](https://arxiv.org/pdf/1707.00192.pdf) proposed an analogue of multiplier bootstrap. The idea is as follows: along with original SGD iterates, compute for $b = 1, 2, \\ldots, B$,\n",
    "\\begin{equation*}\n",
    "\\theta_t^{(b)} = \\theta_{t-1}^{(b)} - \\eta_tW_t^{(b)}\\nabla f(Z_t; \\theta_{t-1}^{(b)}),\n",
    "\\end{equation*}\n",
    "where $W_t^{(b)}, t\\ge0, b\\ge1$ are independent and identically distributed non-negative random variables with mean and variance equal to $1$. If $\\eta_t = \\eta t^{-\\alpha}$ for some $\\eta > 0$ and $\\alpha\\in(1/2, 1)$, then Theorem 1 \\& 2 of [Fang, Xu, and Yang](https://arxiv.org/pdf/1707.00192.pdf) imply in some cases that $T^{-1/2}(\\bar{\\theta}_T^{(b)} - \\bar{\\theta}_T)$ has asymptotically the same distribution as $T^{-1/2}(\\bar{\\theta}_T - \\theta^{\\star})$. Hence, one can use the empirical variance of $\\bar{\\theta}_T^{(b)}, b = 1, 2, \\ldots, B$ as a consistent estimator of $J^{-1}KJ^{-1}$ and use the Wald intervals. Alternatively, one can use the quantiles of $\\bar{\\theta}_T^{(b)}, b = 1, 2, \\ldots, B$ to construct an analogue of quantile bootstrap confidence intervals. Note that this method requires $(B+1)$ times the computational cost of running an SGD. But one does not need to store the whole data because the perturbed iterates $\\theta_t^{(b)}$ can be computed sequentially.\n",
    "### HulC Inference for SGD\n",
    "We now show that without any additional computational cost, one can perform statistical inference using HulC. We already have that the average of the iterates of the SGD is asymptotically Gaussian with mean zero which implies that it is median unbiased. Hence, HulC can be applied with $\\Delta = 0$ to obtain a valid confidence interval for the coordinates of $\\theta^{\\star}$. Note that one can split the data into a fixed number of batches sequentially without the knowledge of the time horizon $T$. It is worth pointing out that such asymptotic normality results hold true for the other versions of SGD mentioned in the introduction. For all these algorithms, HulC can be applied with $\\Delta = 0$. For $\\Delta = 0$ and a 95\\% confidence interval, HulC randomizes between $5$ and $6$ splits of the data. The randomization probability is independent of the data. For any given dataset, we find the number of splits and then pass data sequentially to different SGD paths. Then report the minimum and maximum of the obtained estimators. This is implemented in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7c2dc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "5"
      ],
      "text/latex": [
       "5"
      ],
      "text/markdown": [
       "5"
      ],
      "text/plain": [
       "[1] 5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Finding randomized B. This is the actually number of splits used for constructing the convex hull.\n",
    "## For the purposes of SGD inference, we only need Delta = 0 and t = 0.\n",
    "## If randomize = FALSE, then the method always uses 6 splits with Delta = 0, t = 0. \n",
    "## This can have coverage between 1 - alpha and 1 - alpha/2.\n",
    "## The advantage would be that the finite sample median bias can be as large as 0.1.\n",
    "find_randomize_B <- function(alpha, Delta = 0.0, t = 0.0, randomize = TRUE){\n",
    "    if(Delta == 0.5 && t == 0){\n",
    "        stop(\"Delta is 0.5 and t = 0. The estimator lies only on one side of the parameter!\")\n",
    "    }\n",
    "    B_low <- max(ceiling(log((2 + 2*t)/alpha, base = 2 + 2*t)), ceiling(log((1 + t)/alpha, base = (2 + 2*t)/(1 + 2*Delta))))\n",
    "    B_up <- ceiling(log((2 + 2*t)/alpha, base = (2 + 2*t)/(1 + 2*Delta)))\n",
    "    Q <- function(B){\n",
    "        ((1/2 - Delta)^B + (1/2 + Delta)^B)*(1 + t)^(-B + 1)\n",
    "    }\n",
    "    for(B in B_low:B_up){\n",
    "        if(Q(B) <= alpha)\n",
    "            break\n",
    "    }\n",
    "    B1 <- B\n",
    "    if(randomize){\n",
    "        B0 <- B1 - 1\n",
    "        p1 <- Q(B1)\n",
    "        p0 <- Q(B0)\n",
    "        U <- runif(1)\n",
    "        tau <- (alpha - Q(B1))/(Q(B0) - Q(B1))\n",
    "        B <- B0*(U <= tau) + B1*(U > tau)\n",
    "    }\n",
    "    return(B)\n",
    "}\n",
    "\n",
    "find_randomize_B(0.05, 0, 0, randomize = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00fe832",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SGD with splitting.\n",
    "library(\"sgd\")\n",
    "vignette(package=\"sgd\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
