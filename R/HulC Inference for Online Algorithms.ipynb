{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c757715",
   "metadata": {},
   "source": [
    "# HulC for Online Algorithms\n",
    "## Introduction\n",
    "For big data problems, M-estimators are often obtained using online algorithms such as the stochastic gradient descent (SGD) or many of its variants. These algorithms have a low cost per iteration and only takes $n$ iterations where $n$ is the number of observations. The vanilla SGD has been extended and generalized in several ways for improvement. For example, the implicit SGD of [Toulis and Airoldi](https://drive.google.com/file/d/1aC6ZHZF-_YDw4T8cwYlmfwSqL9-LREyf/view) (2017, Annals of Statistics) provides implicit updates on the parameter values and brings more stability compared to the vanilla SGD. The ROOT-SGD method of [Li, Mou, Wainwright, and Jordon](https://arxiv.org/pdf/2008.12690.pdf) (2020, ArXiv:2008.12690) improves the vanilla SGD in terms of its finite sample behavior as well as reduce the order of higher order terms for asymptotic normality. The gradient-free SGD method of [Chen, Lai, Li, and Zhang](https://arxiv.org/pdf/2102.03389.pdf) (2021, ArXiv:2102.03389) relaxes the requirement explicitly known unbiased estimator of gradient for SGD. They only use function values to compute the gradient by Gaussian perturbation. These online algorithms nicely tackle the problem of estimation from a computational perspective. However, inference via confidence intervals using these online algorithms is relatively not as well understood. For all the algorithms mentioned above including the vanilla SGD, it is known that under certain regularity conditions, the average of the iterates has an asymptotic Gaussian distribution centered at the true parameter value. For this result for vanilla SGD, see [Polyak and Juditsky](http://www.meyn.ece.ufl.edu/archive/spm_files/Courses/ECE555-2011/555media/poljud92.pdf) (1992, SIAM J. Control and Optimization). The inference problem based on these online algorithms has received a lot of attention in the recent times. There are two main lines of research:\n",
    "- Estimating the asymptotic covariance matrix and construct the usual Wald intervals;\n",
    "- Run a parallel perturbed online algorithms so as to replicate the distribution of the original estimator. This is similar to multiplier bootstrap.\n",
    "\n",
    "## SGD and asymptotic normality\n",
    "Let us now discuss the vanilla SGD and discuss its asymptotic normality. Let $\\theta^{\\star}\\in\\mathbb{R}^d$ be defined as\n",
    "\\begin{equation*}\n",
    "\\theta^{\\star} := \\underset{\\theta\\in\\mathbb{R}^d}{\\text{argmin}} F(\\theta),\\quad\\mbox{where}\\quad F(\\theta) = \\mathbb{E}[f(Z; \\theta)],\n",
    "\\end{equation*}\n",
    "for a random variable $Z$ in some measurable space. Let $Z_1, Z_2, \\ldots$ be a sequence of independent and identically distributed random variables observed sequentially. If $\\theta_0$ denotes a given starting point, the SGD iterate at the $t$-th iterate is given by\n",
    "\\begin{equation*}\n",
    "\\theta_t = \\theta_{t-1} - \\eta_t\\nabla f(Z_t; \\theta_{t-1}),\n",
    "\\end{equation*}\n",
    "where $\\nabla f(z; \\theta) = \\partial f(z; \\theta)/\\partial\\theta$ denotes the gradient of $\\theta\\mapsto f(z; \\theta)$.\n",
    "In general, the step size $\\eta_t$ is chosen to decrease with $t$. At time $T$, the algorithm can return either the last iterate $\\theta_T$ or the average iterate $\\bar{\\theta}_T = T^{-1}\\sum_{t=1}^T \\theta_t$. The averaging is called Polyak--Ruppert averaging. The results of [Polyak and Juditsky](http://www.meyn.ece.ufl.edu/archive/spm_files/Courses/ECE555-2011/555media/poljud92.pdf) imply that the average iterate is a \"better\" estimator of $\\theta^{\\star}$ than the last iterate. In the following, we only discuss the properties of the average iterate $\\bar{\\theta}_T$. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
