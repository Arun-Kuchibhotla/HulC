{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e2fea21",
   "metadata": {},
   "source": [
    "# HulC for shape constrained regression\n",
    "## Univariate Monotone Regression \n",
    "Suppose $(X_i, Y_i)\\in\\mathbb{R}\\times\\mathbb{R}, 1\\le i\\le n$  are independent observations from the non-parametric model\n",
    "\\begin{equation*}\n",
    "Y_i = f_0(X_i) + \\xi_i,\\quad\\mathbb{E}[\\xi_i|X_i] = 0.\n",
    "\\end{equation*}\n",
    "In the traditional non-parametric model, there are no functional form restrictions on $f_0$ except for some smoothness assumptions. In the monotone regression problem, we do not impose smoothness assumptions but require that $f_0$ be a non-decreasing function. Two important aspects to note about the model above: (1) we do not require the errors $\\xi_i$ to be homoscedastic or independent of the covariates, and (2) we do not require $X_i$'s to be on an equi-spaced grid. They can either be random or fixed. \n",
    "### Problem\n",
    "We consider the problem of constructing confidence interval for $f_0(x^*)$ where $x^*$ is a point in the support of the distribution of the $X_i$'s.\n",
    "\n",
    "## LSE and pointwise asymptotics\n",
    "Before we consider the application of HulC, we need to consider an estimator and its median bias properties. A natural estimator for $f_0$ is obtained via least squares\n",
    "\\begin{equation*}\n",
    "\\widehat{f} ~:=~ \\underset{f: \\text{non-decreasing}}{\\text{argmin}}\\,\\frac{1}{n}\\sum_{i=1}^n (Y_i - f(X_i))^2.\n",
    "\\end{equation*}\n",
    "Note that this is an infinite dimensional convex optimization problem. Because the objective function depends only on $f(X_i), 1\\le i\\le n$, it can be converted into an $n$-dimensional constrained quadratic programming problem. There is an efficient algorithm, called PAVA, to solve this problem and is implemented via the R function `isoreg`. In contrast to many of the commonly used non-parametric estimators, $\\widehat{f}$ is fully automated and does not involve any tuning parameters.\n",
    "\n",
    "### Pointwise asymptotics\n",
    "Wright (1981, AoS, The asymptotic behavior of monotone regression estimates) proved the pointwise asymptotic distribution of $\\widehat{f}(\\cdot)$. The simplest case of this is when $f_0$ is continuously differentiable at the point $x^*$. If $f_0$ is continuously differentiable at $x^*$ and $f_0'(x^*) > 0$, then\n",
    "\\begin{equation*}\n",
    "n^{1/3}(\\widehat{f}(x^*) - f_0(x^*)) ~\\overset{d}{\\to}~ \\left(\\frac{4\\sigma^2(x^*)f_0'(x^*)}{h(x^*)}\\right)^{1/3}\\mathbb{C},\n",
    "\\end{equation*}\n",
    "where $\\sigma^2(x) = \\mbox{Var}(Y_i|X_i = x),$ $h(\\cdot)$ is the density of $X_i$'s, and $\\mathbb{C}$ is a random variable with the Chernoff distribution. $\\mathbb{C}$ is a symmetric (about $0$) random variable that is obtained as the minimizer of a drifted two-sided Brownian motion starting at $0$. It is important for this result that $f_0(x^*) > 0$. Note that $f_0$ being non-decreasing, $f_0'(x^*) \\ge 0$. If $X_i$'s are fixed, then $h(\\cdot)$ should be thought of the derivative of the limiting empirical distribution of the covariates.\n",
    " \n",
    "The more general result allows for non-differentiable of $f_0$ at $x^*$ as well as higher order smoothness. Suppose $f_0$ at $x^*$ satisfies\n",
    "\\begin{equation*}\n",
    "\\frac{|f_0(x) - f_0(x^*)|}{|x - x^*|^{\\beta}} ~=~ A(1 + o(1))\\quad\\mbox{as}\\quad x\\to x^*,\n",
    "\\label{eq:smoothness}\\tag{1}\n",
    "\\end{equation*}\n",
    "for some $1/2 \\le \\beta < \\infty$, then\n",
    "\\begin{equation*}\n",
    "n^{\\beta/(2\\beta + 1)}(\\widehat{f}(x^*) - f_0(x^*)) ~\\overset{d}{\\to}~ \\left(\\frac{\\sigma^2(x^*)A^{1/\\beta}}{h(x^*)(\\beta + 1)^{1/\\beta}}\\right)^{\\beta/(2\\beta + 1)}\\mathbb{C}_{\\beta},\n",
    "\\end{equation*}\n",
    "where $\\mathbb{C}_{\\beta}$ is the slope at zero of the greatest convex minorant of $W(t) + |t|^{\\beta + 1}$, where $W(\\cdot)$ is the two-sided Wiener-Levy process with variance one per unit time. The result in Wright (1981, AoS) does not require $\\beta \\ge 1/2$, but we do because we state the result for both random and fixed design. See Section 3 of Wright (1981, AoS) for details.\n",
    "\n",
    "If $\\beta$ is an integer, then the smoothness condition \\eqref{eq:smoothness} is equivalent to\n",
    "\\begin{equation*}\n",
    "f_0'(x^*) = \\cdots = f_0^{(\\beta - 1)}(x^*) = 0,\\quad\\mbox{and}\\quad \\frac{f_0^{(\\beta)}(x^*)}{\\beta!} = A.\n",
    "\\end{equation*}\n",
    "This shows that the condition \\eqref{eq:smoothness} is more of a local flatness condition rather than a local smoothness condition. Furthermore, the result above shows that the monotone LSE attains adaptive local rate of convergence as the local flatness of the function increases. Note that for $1/2 \\le \\beta < \\infty$, the limiting distribution is a product of two quantities: a scalar non-random factor depending only on $f_0$ and $x^*$, and a random variable $\\mathbb{C}_{\\beta}$ independent of $f_0$ and $x^*$. \n",
    "\n",
    "Both the pointwise limiting distributions results require additional regularity conditions including a Lindeberg type condition. We refer the reader to Theorem 1 of Wright (1981, AoS) for more details. Observe that both the results stated above do not cover the case where $f_0$ is flat at $x^*$ (i.e., $\\beta = \\infty$ in \\eqref{eq:smoothness}). If $f_0$ has a flat piece around $x^*$, then $n^{1/2}(\\widehat{f}(x^*) - f_0(x^*))$ has a non-degenerate limiting distribution. The limiting distribution here is complicated and does not have the multiplicative form that holds for $\\beta < \\infty$. \n",
    "\n",
    "**Some Comments.**\n",
    "- The rate of convergence and the limiting distribution depends on the local flatness of $f_0$.\n",
    "- The limiting distribution depends on additional unknown nuisance components such as the design density and heteroscedasticity. In general, estimation of these nuisance components involves tuning parameters, while estimation of $f_0(x^*)$ does not require any.\n",
    "- Traditional methods of inference such as Wald's, bootstrap, and subsampling fall short in various ways. Firstly, Wald interval is hard to implement here because it involve estimation of multiple nuisance components and further \"good\" estimation of these nuisance components require some smoothness conditions on them which is not required for estimation of $f_0(x^*)$. Secondly, bootstrap is known to be inconsistent with the LSE (see Guntuboyina and Sen (2018, Statistical Science)), but bootstrap with smoothed LSE is known to be consistent. However, smoothing the LSE involves tuning parameters. Finally, subsampling usually requires the knowledge of the rate of convergence of the estimator which is not available here because $\\beta$ is unknown.\n",
    "\n",
    "All the comments above also apply to other shape constrained models. For example, for convex regression model where $f_0$ is constrained to be convex. They also apply to current status models. Further, they also apply to multivariate monotone regression model with the LSE replaced by the block min-max estimator; see Han and Zhang (2020, AoS, Limit distribution theory for block estimators in multiple isotonic regression) for details. Recently, Deng, Han, and Zhang (2020, AoS, Confidence intervals for multiple isotonic regression and other monotone models) have derived an asymptotically \"pivotal.\" This theory is currently only available for errors independent of covariates and the covariates, if random, are uniformly distribution. Further, the pivotal limiting distribution still depends on the unknown variance of the errors. \n",
    "\n",
    "## Median bias of the LSE\n",
    "Let us now consider the median bias of the LSE. In the simplest case where $f_0'(x^*) > 0$, then\n",
    "\\begin{equation*}\n",
    "\\mathbb{P}\\left(n^{1/3}(\\widehat{f}(x^*) - f_0(x^*)) \\le 0\\right) ~\\to~ \\mathbb{P}\\left(\\left(\\frac{4\\sigma^2(x^*)f_0'(x^*)}{h(x^*)}\\right)^{1/3}\\mathbb{C} \\le 0\\right) = \\mathbb{P}(\\mathbb{C} \\le 0) = \\frac{1}{2}.\n",
    "\\end{equation*}\n",
    "Therefore, $\\widehat{f}(x^*)$ as an estimator of $f_0(x^*)$ is asymptotically median unbiased. In the complicated case where \\eqref{eq:smoothness} is satisfied, the LSE satisfies\n",
    "\\begin{equation*}\n",
    "\\mathbb{P}\\left(n^{\\beta/(2\\beta + 1)}(\\widehat{f}(x^*) - f_0(x^*)) \\le 0\\right) ~\\to~ \\mathbb{P}(\\mathbb{C}_{\\beta} \\le 0).\n",
    "\\end{equation*}\n",
    "The distribution of $\\mathbb{C}_{\\beta}$ is not well-understood and it is not known if $\\mathbb{P}(\\mathbb{C}_{\\beta} \\le 0) = 1/2$. The asymptotic median bias of $\\widehat{f}(x^*)$ under \\eqref{eq:smoothness} is given by\n",
    "\\begin{equation*}\n",
    "\\mbox{Med-bias}_{f_0(x^*)}(\\widehat{f}(x^*)) ~\\to~ \\left|\\frac{1}{2} - \\mathbb{P}(\\mathbb{C}_{\\beta} \\le 0)\\right|.\n",
    "\\label{eq:asymp-med-bias}\\tag{2}\n",
    "\\end{equation*}\n",
    "To get an understanding of the right hand side, we will generate 1000 times data of size $10^6$ from the model\n",
    "\\begin{equation}\n",
    "Y_i = \\mbox{sign}(X_i)|X_i|^{\\beta} + \\xi_i,\n",
    "\\end{equation}\n",
    "where $\\xi_i\\sim N(0, 1)$ and $X_i\\sim \\mbox{Unif}[-1, 1]$. Note that the function $f_0(x) = \\mbox{sign}(x)|x|^{\\beta}$ is increasing and $f_0$ satisfies \\eqref{eq:smoothness} at $x^* = 0$. Hence the right hand side of \\eqref{eq:asymp-med-bias} by considering the proportion among 1000 replication of when $\\widehat{f}(x^*)$ \\le 0$. The following code estimates the right hand side of \\eqref{eq:asymp-med-bias} for $\\beta$ ranging from $1/2$ to $50$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578dc5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The following is time taking.\n",
    "beta_seq <- seq(1/2, 50, length = 200)\n",
    "nrep <- 1000\n",
    "nsamp <- 1e06\n",
    "ISO_mat <- matrix(0, ncol = length(beta_seq), nrow = nrep)\n",
    "for(beta_idx in 1:length(beta_seq)){\n",
    "    beta <- beta_seq[beta_idx]\n",
    "    foo <- function(x){\n",
    "        sign(x)*abs(x)^beta\n",
    "    }\n",
    "    for(idx in 1:nrep){\n",
    "        x_seq <- seq(-1, 1, length = nsamp)\n",
    "        y_seq <- foo(x_seq) + rnorm(nsamp)\n",
    "        ## computing fhat\n",
    "        tmp <- isoreg(x_seq, y_seq)\n",
    "        ## computing fhat(0)\n",
    "        ISO_mat[idx, beta_idx] <- approxfun(sort(tmp$x), tmp$yf, method=\"constant\", rule = 2)(0)\n",
    "    }    \n",
    "}\n",
    "print(\"computing ISO_mat is done!!\")\n",
    "## Computing the median bias\n",
    "median_bias <- abs(colMeans(sign(ISO_mat))/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229be9a8",
   "metadata": {},
   "source": [
    "The output (`median_bias`) shows that $|1/2 - \\mathbb{P}(\\mathbb{C}_{\\beta} \\le 0)|$ is close to zero for $\\beta\\in[1/2, 50]$. The maximum in the estimated median bias is $0.05$. This means that HulC can be applied with $\\Delta = 0$ to construct a confidence interval for $f_0(x^*)$ for any $\\beta\\in[1/2, \\infty). Hence, for a non-decreasing function without any flat pieces HulC with $\\Delta = 0$ yields an asymptotically valid confidence interval without having to estimate the heteroscedasticity or design density or the derivatives of $f_0$. The median bias for $\\beta = \\infty$ would depend very crucially on $f_0$ and $x^*$ and will be discussed later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45388553",
   "metadata": {},
   "source": [
    "## HulC for functions without flat pieces\n",
    "In the following, we first provide simulation experiments to show the performance of HulC for pointwise inference for $f_0$ that do not include any flat pieces. Consider $(X_i, Y_i), 1\\le i\\le n$ from the model\n",
    "\\begin{equation*}\n",
    "Y_i = \\exp(2X_i) + \\xi_i\\quad\\mbox{and}\\quad X_i\\sim\\mbox{Unif}[0, 1],\\quad\\xi_i|X_i \\sim N(0, 1).\n",
    "\\end{equation*}\n",
    "Here $f_0(x) = \\exp(2x)$ and $f_0'(x) = 2e^{2x} > 0$ for all $x\\in[0, 1]$. This example is considered in Section 4.2.1 of Deng, Han, and Zhang (2020, AoS). In the following, we show the coverage as well as width properties of HulC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "428f94e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in B_low:B_up: NA/NaN argument\n",
     "output_type": "error",
     "traceback": [
      "Error in B_low:B_up: NA/NaN argument\nTraceback:\n",
      "1. HulC(cbind(x_seq, y_seq), estimate, alpha = alpha * length(x_grid), \n .     Delta = 0, dim = length(x_grid))",
      "2. HulC1d(data, foo, alpha = alpha/dim, Delta = Delta[idx], randomize = randomize)",
      "3. solve_for_B(alpha = alpha, Delta = Delta, t = 0)"
     ]
    }
   ],
   "source": [
    "### HulC.R contains all the functions to the HulC procedure.\n",
    "source(\"HulC.R\")\n",
    "\n",
    "### loading necessary packages plotting.\n",
    "library(\"repr\")\n",
    "\n",
    "## One replication\n",
    "alpha <- 0.05\n",
    "x_grid <- seq(0, 1, length = 50)\n",
    "x_seq <- sort(runif(100))\n",
    "f0 <- function(x){ exp(2*x) }\n",
    "y_seq <- f0(x_seq) + rnorm(100)\n",
    "estimate <- function(dat){\n",
    "  tmp <- isoreg(dat[,1], dat[,2])\n",
    "  approxfun(sort(tmp$x), tmp$yf, method=\"constant\", rule = 2)(x_grid)\n",
    "}\n",
    "tmp <- HulC(cbind(x_seq, y_seq), estimate, alpha = alpha*length(x_grid), \n",
    "            Delta = 0, dim = length(x_grid))$CI\n",
    "plot(x_seq, f0(x_seq), type = 'l')\n",
    "points(x_grid, tmp[,1], col = \"red\", pch = \"*\")\n",
    "points(x_grid, tmp[,2], col = \"blue\", pch = \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46c26738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>ci_lwr</th><th scope=col>ci_upr</th><th scope=col>x</th><th scope=col>nsamp</th><th scope=col>rep</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0         </td><td>0         </td><td>0.00000000</td><td>100       </td><td>1         </td></tr>\n",
       "\t<tr><td>0         </td><td>0         </td><td>0.01010101</td><td>100       </td><td>1         </td></tr>\n",
       "\t<tr><td>0         </td><td>0         </td><td>0.02020202</td><td>100       </td><td>1         </td></tr>\n",
       "\t<tr><td>0         </td><td>0         </td><td>0.03030303</td><td>100       </td><td>1         </td></tr>\n",
       "\t<tr><td>0         </td><td>0         </td><td>0.04040404</td><td>100       </td><td>1         </td></tr>\n",
       "\t<tr><td>0         </td><td>0         </td><td>0.05050505</td><td>100       </td><td>1         </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " ci\\_lwr & ci\\_upr & x & nsamp & rep\\\\\n",
       "\\hline\n",
       "\t 0          & 0          & 0.00000000 & 100        & 1         \\\\\n",
       "\t 0          & 0          & 0.01010101 & 100        & 1         \\\\\n",
       "\t 0          & 0          & 0.02020202 & 100        & 1         \\\\\n",
       "\t 0          & 0          & 0.03030303 & 100        & 1         \\\\\n",
       "\t 0          & 0          & 0.04040404 & 100        & 1         \\\\\n",
       "\t 0          & 0          & 0.05050505 & 100        & 1         \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| ci_lwr | ci_upr | x | nsamp | rep |\n",
       "|---|---|---|---|---|\n",
       "| 0          | 0          | 0.00000000 | 100        | 1          |\n",
       "| 0          | 0          | 0.01010101 | 100        | 1          |\n",
       "| 0          | 0          | 0.02020202 | 100        | 1          |\n",
       "| 0          | 0          | 0.03030303 | 100        | 1          |\n",
       "| 0          | 0          | 0.04040404 | 100        | 1          |\n",
       "| 0          | 0          | 0.05050505 | 100        | 1          |\n",
       "\n"
      ],
      "text/plain": [
       "  ci_lwr ci_upr x          nsamp rep\n",
       "1 0      0      0.00000000 100   1  \n",
       "2 0      0      0.01010101 100   1  \n",
       "3 0      0      0.02020202 100   1  \n",
       "4 0      0      0.03030303 100   1  \n",
       "5 0      0      0.04040404 100   1  \n",
       "6 0      0      0.05050505 100   1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre class=language-r><code>function (data, estimate, alpha = 0.05, Delta = 0, dim = 1, randomize = TRUE) \n",
       "NULL</code></pre>"
      ],
      "text/latex": [
       "\\begin{minted}{r}\n",
       "function (data, estimate, alpha = 0.05, Delta = 0, dim = 1, randomize = TRUE) \n",
       "NULL\n",
       "\\end{minted}"
      ],
      "text/markdown": [
       "```r\n",
       "function (data, estimate, alpha = 0.05, Delta = 0, dim = 1, randomize = TRUE) \n",
       "NULL\n",
       "```"
      ],
      "text/plain": [
       "function (data, estimate, alpha = 0.05, Delta = 0, dim = 1, randomize = TRUE) \n",
       "NULL"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Initiating matrices and vectors\n",
    "## x_grid is the x^* values\n",
    "x_grid <- seq(0, 1, length = 100)\n",
    "## number of replications\n",
    "nrep <- 1000\n",
    "## different sample sizes\n",
    "nsamp_seq <- c(100, 250)\n",
    "\n",
    "final_grid <- expand.grid(x_grid, nsamp_seq, 1:nrep)\n",
    "colnames(final_grid) <- c(\"x\", \"nsamp\", \"rep\")\n",
    "hulc_final_grid <- data.frame(ci_lwr = rep(0, nrow(final_grid)),\n",
    "                             ci_upr = rep(0, nrow(final_grid)),\n",
    "                             x = final_grid[,1],\n",
    "                             nsamp = final_grid[,2],\n",
    "                             rep = final_grid[,3])\n",
    "head(hulc_final_grid)\n",
    "args(HulC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fca257",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting seed for reproducibility\n",
    "set.seed(2021)\n",
    "alpha <- 0.05\n",
    "f0 <- function(x){ exp(2*x) }\n",
    "for(idx in 1:length(final_grid)){\n",
    "    x_seq <- runif(final_grid[idx,2])\n",
    "    y_seq <- f0(x_seq) + rnorm(final_grid[idx, 2])\n",
    "    estimate <- function(dat){\n",
    "        tmp <- isoreg(dat[,1], dat[,2])\n",
    "        approxfun(sort(tmp$x), tmp$yf, method=\"constant\", rule = 2)(final_grid[idx, 1])\n",
    "    }\n",
    "    hulc_final_grid[idx, 1:2] <- HulC(cbind(x_seq, y_seq), estimate, alpha = alpha, Delta = 0)$CI\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
